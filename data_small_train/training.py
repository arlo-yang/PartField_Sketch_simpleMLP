#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å­˜é«˜æ•ˆSOTAæ¨¡å‹è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. ä½¿ç”¨æˆ‘ä»¬çš„loader.pyåŠ è½½æ•°æ®
2. è®­ç»ƒå†…å­˜é«˜æ•ˆSOTAæ¨¡å‹
3. æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦
4. å®æ—¶ç›‘æ§å’Œå¯è§†åŒ–
5. æ¨¡å‹ä¿å­˜å’Œæ¢å¤
"""

import os
import sys
import time
import argparse
import yaml
import numpy as np
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from torch.utils.tensorboard import SummaryWriter

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from loader import PartSegmentationDataset, get_dataloader
from model import create_model, ConfidenceWeightedLoss, FocalLoss

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class Trainer:
    """å†…å­˜é«˜æ•ˆSOTAæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®éšæœºç§å­
        set_seed(config.get('seed', 42))
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(log_dir=self.output_dir / 'tensorboard')
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦
        self.use_amp = config.get('use_amp', True) and torch.cuda.is_available()
        self.scaler = GradScaler() if self.use_amp else None
        
        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡
        self.best_val_acc = 0.0
        self.best_val_f1 = 0.0
        
        print(f"è®­ç»ƒè®¾å¤‡: {self.device}")
        print(f"æ··åˆç²¾åº¦: {'å¯ç”¨' if self.use_amp else 'ç¦ç”¨'}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def build_datasets(self):
        """æ„å»ºæ•°æ®é›†"""
        print("æ„å»ºæ•°æ®é›†...")
        
        data_config = self.config['data']
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset = PartSegmentationDataset(
            data_dir=data_config['data_dir'],
            split='train',
            config=data_config
        )
        
        self.val_dataset = PartSegmentationDataset(
            data_dir=data_config['data_dir'],
            split='val',
            config=data_config
        )
        
        self.test_dataset = PartSegmentationDataset(
            data_dir=data_config['data_dir'],
            split='test',
            config=data_config
        )
        
        # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        self.train_dataset.print_statistics()
        
        print(f"è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(self.test_dataset)} æ ·æœ¬")
    
    def build_dataloaders(self):
        """æ„å»ºæ•°æ®åŠ è½½å™¨"""
        print("æ„å»ºæ•°æ®åŠ è½½å™¨...")
        
        dataloader_config = self.config['dataloader']
        
        self.train_loader = get_dataloader(self.train_dataset, dataloader_config)
        
        # éªŒè¯å’Œæµ‹è¯•ä¸éœ€è¦shuffle
        val_config = dataloader_config.copy()
        val_config['shuffle'] = False
        self.val_loader = get_dataloader(self.val_dataset, val_config)
        self.test_loader = get_dataloader(self.test_dataset, val_config)
        
        print(f"è®­ç»ƒæ‰¹æ¬¡: {len(self.train_loader)}")
        print(f"éªŒè¯æ‰¹æ¬¡: {len(self.val_loader)}")
        print(f"æµ‹è¯•æ‰¹æ¬¡: {len(self.test_loader)}")
    
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("æ„å»ºæ¨¡å‹...")
        
        model_config = self.config['model']
        self.model = create_model(model_config)
        self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°
        loss_config = self.config.get('loss', {})
        loss_type = loss_config.get('type', 'confidence_weighted')
        
        if loss_type == 'confidence_weighted':
            self.criterion = ConfidenceWeightedLoss(
                base_loss=loss_config.get('base_loss', 'focal'),
                conf_weight=loss_config.get('conf_weight', 0.3),
                alpha=loss_config.get('alpha', 0.25),
                gamma=loss_config.get('gamma', 2.0)
            )
        elif loss_type == 'focal':
            self.criterion = FocalLoss(
                alpha=loss_config.get('alpha', 0.25),
                gamma=loss_config.get('gamma', 2.0)
            )
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        print(f"æŸå¤±å‡½æ•°: {loss_type}")
    
    def build_optimizer(self):
        """æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        print("æ„å»ºä¼˜åŒ–å™¨...")
        
        opt_config = self.config['optimizer']
        
        # ä¼˜åŒ–å™¨
        if opt_config['type'] == 'adamw':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=opt_config['lr'],
                weight_decay=opt_config.get('weight_decay', 0.01),
                betas=opt_config.get('betas', (0.9, 0.999))
            )
        else:
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=opt_config['lr'],
                weight_decay=opt_config.get('weight_decay', 0.01)
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.config.get('scheduler', {})
        scheduler_type = scheduler_config.get('type', 'cosine')
        
        if scheduler_type == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['training']['epochs'],
                eta_min=scheduler_config.get('eta_min', 1e-6)
            )
        elif scheduler_type == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=scheduler_config.get('step_size', 30),
                gamma=scheduler_config.get('gamma', 0.1)
            )
        else:
            self.scheduler = None
        
        print(f"ä¼˜åŒ–å™¨: {opt_config['type']}")
        print(f"å­¦ä¹ ç‡è°ƒåº¦: {scheduler_type}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, (features, labels, sample_info) in enumerate(pbar):
            features = features.to(self.device)
            labels = labels.to(self.device)
            confidence = features[:, 448:449]  # æå–confidence
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            if self.use_amp:
                with autocast():
                    if self.config['training'].get('use_checkpointing', False):
                        logits = self.model.forward_with_checkpointing(features)
                    else:
                        logits = self.model(features)
                    
                    if isinstance(self.criterion, ConfidenceWeightedLoss):
                        loss = self.criterion(logits, labels, confidence)
                    else:
                        loss = self.criterion(logits, labels)
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config['training'].get('max_grad_norm', 0) > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['training']['max_grad_norm']
                    )
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                if self.config['training'].get('use_checkpointing', False):
                    logits = self.model.forward_with_checkpointing(features)
                else:
                    logits = self.model(features)
                
                if isinstance(self.criterion, ConfidenceWeightedLoss):
                    loss = self.criterion(logits, labels, confidence)
                else:
                    loss = self.criterion(logits, labels)
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config['training'].get('max_grad_norm', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['training']['max_grad_norm']
                    )
                
                self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = logits.argmax(dim=1)
            total_correct += (pred == labels).sum().item()
            total_samples += labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / (batch_idx + 1)
            avg_acc = total_correct / total_samples
            pbar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'Acc': f'{avg_acc:.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
            
            # è®°å½•åˆ°TensorBoard
            global_step = epoch * len(self.train_loader) + batch_idx
            if batch_idx % 100 == 0:
                self.writer.add_scalar('Train/Loss', loss.item(), global_step)
                self.writer.add_scalar('Train/Accuracy', avg_acc, global_step)
                self.writer.add_scalar('Train/LR', self.optimizer.param_groups[0]['lr'], global_step)
        
        return total_loss / len(self.train_loader), total_correct / total_samples
    
    @torch.no_grad()
    def validate(self, dataloader, split_name='Val'):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        all_preds = []
        all_labels = []
        
        pbar = tqdm(dataloader, desc=f'{split_name}')
        
        for features, labels, sample_info in pbar:
            features = features.to(self.device)
            labels = labels.to(self.device)
            confidence = features[:, 448:449]
            
            if self.use_amp:
                with autocast():
                    logits = self.model(features)
                    if isinstance(self.criterion, ConfidenceWeightedLoss):
                        loss = self.criterion(logits, labels, confidence)
                    else:
                        loss = self.criterion(logits, labels)
            else:
                logits = self.model(features)
                if isinstance(self.criterion, ConfidenceWeightedLoss):
                    loss = self.criterion(logits, labels, confidence)
                else:
                    loss = self.criterion(logits, labels)
            
            total_loss += loss.item()
            pred = logits.argmax(dim=1)
            total_correct += (pred == labels).sum().item()
            total_samples += labels.size(0)
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / len(all_preds) * labels.size(0)
            avg_acc = total_correct / total_samples
            pbar.set_postfix({
                'Loss': f'{avg_loss:.4f}',
                'Acc': f'{avg_acc:.4f}'
            })
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
        
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
        
        return {
            'loss': total_loss / len(dataloader),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def save_checkpoint(self, epoch, metrics, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'metrics': metrics,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.output_dir / 'latest_checkpoint.pth')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, self.output_dir / 'best_model.pth')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val F1: {metrics['val_f1']:.4f})")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.scaler and checkpoint['scaler_state_dict']:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        return checkpoint['epoch'], checkpoint['metrics']
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("å¼€å§‹è®­ç»ƒ...")
        
        # æ„å»ºæ‰€æœ‰ç»„ä»¶
        self.build_datasets()
        self.build_dataloaders()
        self.build_model()
        self.build_optimizer()
        
        # è®­ç»ƒé…ç½®
        epochs = self.config['training']['epochs']
        start_epoch = 0
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config['training'].get('resume', False):
            checkpoint_path = self.output_dir / 'latest_checkpoint.pth'
            if checkpoint_path.exists():
                start_epoch, _ = self.load_checkpoint(checkpoint_path)
                print(f"ä»epoch {start_epoch} æ¢å¤è®­ç»ƒ")
        
        # è®­ç»ƒå†å²
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        val_f1s = []
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(start_epoch, epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{epochs}")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_metrics = self.validate(self.val_loader, 'Val')
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            val_losses.append(val_metrics['loss'])
            val_accs.append(val_metrics['accuracy'])
            val_f1s.append(val_metrics['f1'])
            
            # TensorBoardè®°å½•
            self.writer.add_scalar('Epoch/Train_Loss', train_loss, epoch)
            self.writer.add_scalar('Epoch/Train_Acc', train_acc, epoch)
            self.writer.add_scalar('Epoch/Val_Loss', val_metrics['loss'], epoch)
            self.writer.add_scalar('Epoch/Val_Acc', val_metrics['accuracy'], epoch)
            self.writer.add_scalar('Epoch/Val_F1', val_metrics['f1'], epoch)
            
            # æ‰“å°ç»“æœ
            print(f"\nè®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}")
            print(f"éªŒè¯ - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, "
                  f"F1: {val_metrics['f1']:.4f}, Precision: {val_metrics['precision']:.4f}, "
                  f"Recall: {val_metrics['recall']:.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            metrics = {
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_metrics['loss'],
                'val_acc': val_metrics['accuracy'],
                'val_f1': val_metrics['f1']
            }
            
            is_best = val_metrics['f1'] > self.best_val_f1
            if is_best:
                self.best_val_f1 = val_metrics['f1']
                self.best_val_acc = val_metrics['accuracy']
            
            self.save_checkpoint(epoch, metrics, is_best)
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            if (epoch + 1) % 5 == 0:
                self.plot_training_curves(train_losses, train_accs, val_losses, val_accs, val_f1s)
        
        # æœ€ç»ˆæµ‹è¯•
        print(f"\n{'='*60}")
        print("æœ€ç»ˆæµ‹è¯•...")
        print(f"{'='*60}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_checkpoint_path = self.output_dir / 'best_model.pth'
        if best_checkpoint_path.exists():
            self.load_checkpoint(best_checkpoint_path)
        
        test_metrics = self.validate(self.test_loader, 'Test')
        
        print(f"\næœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"Loss: {test_metrics['loss']:.4f}")
        print(f"Accuracy: {test_metrics['accuracy']:.4f}")
        print(f"F1: {test_metrics['f1']:.4f}")
        print(f"Precision: {test_metrics['precision']:.4f}")
        print(f"Recall: {test_metrics['recall']:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_results = {
            'best_val_acc': self.best_val_acc,
            'best_val_f1': self.best_val_f1,
            'test_metrics': test_metrics
        }
        
        with open(self.output_dir / 'final_results.yaml', 'w') as f:
            yaml.dump(final_results, f)
        
        self.writer.close()
        print(f"\nè®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")
    
    def plot_training_curves(self, train_losses, train_accs, val_losses, val_accs, val_f1s):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Lossæ›²çº¿
        axes[0, 0].plot(train_losses, label='Train Loss', color='blue')
        axes[0, 0].plot(val_losses, label='Val Loss', color='red')
        axes[0, 0].set_title('Loss Curves')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Accuracyæ›²çº¿
        axes[0, 1].plot(train_accs, label='Train Acc', color='blue')
        axes[0, 1].plot(val_accs, label='Val Acc', color='red')
        axes[0, 1].set_title('Accuracy Curves')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # F1æ›²çº¿
        axes[1, 0].plot(val_f1s, label='Val F1', color='green')
        axes[1, 0].set_title('F1 Score Curve')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('F1 Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if self.scheduler:
            # ç®€å•æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            axes[1, 1].axhline(y=current_lr, color='orange', linestyle='--', label=f'Current LR: {current_lr:.6f}')
            axes[1, 1].set_title('Learning Rate')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        else:
            axes[1, 1].text(0.5, 0.5, 'No LR Scheduler', ha='center', va='center', transform=axes[1, 1].transAxes)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒå†…å­˜é«˜æ•ˆSOTAæ¨¡å‹")
    parser.add_argument("--config", type=str, default="train_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--resume", action="store_true", help="æ¢å¤è®­ç»ƒ")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.output_dir:
        config['output_dir'] = args.output_dir
    if args.resume:
        config['training']['resume'] = True
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = Trainer(config)
    trainer.train()


if __name__ == "__main__":
    main() 